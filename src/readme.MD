

██████╗  █████╗ ██████╗ ███████╗███╗   ███╗██╗   ██╗██████╗ ███████╗
██╔══██╗██╔══██╗██╔══██╗██╔════╝████╗ ████║██║   ██║██╔══██╗██╔════╝
██████╔╝███████║██████╔╝███████╗██╔████╔██║██║   ██║██████╔╝█████╗
██╔═══╝ ██╔══██║██╔══██╗╚════██║██║╚██╔╝██║██║   ██║██╔══██╗██╔══╝
██║     ██║  ██║██║  ██║███████║██║ ╚═╝ ██║╚██████╔╝██║  ██║██║
╚═╝     ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝     ╚═╝ ╚═════╝ ╚═╝  ╚═╝╚═╝



This package contains parSMURF, a High Performance Computing imbalance-aware machine learning tool for the genome-wide detection of pathogenic variants.
parSMURF is a fast and scalable C++ implementation of the HyperSMURF algorithm - hyper-ensemble of SMOTE Undersampled Random Forests - an ensemble approach explicitly designed to deal with the huge imbalance between deleterious and neutral variants.

The algorithm is outlined in the following papers:
A. Petrini, M. Mesiti, M. Schubach, M. Frasca, D. Danis, M. Re, G. Grossi, T. Castrignanò, P. N. Robinson and G. Valentini, "parSMURF, a High Performance Computing tool for the genome-wide detection of pathogenic variants OR parSMURF, a High Performance Computing tool for supervised big data analysis in Genomic Medicine", <TBD>

Schubach, Matteo Re, Peter N. Robinson & Giorgio Valentini, "Imbalance-Aware Machine Learning for Predicting Rare and Common Disease-Associated Non-Coding Variants", Scientific Reports, 2017/06/07
https://www.nature.com/articles/s41598-017-03011-5

Two variants of parSMURF are currently available in this repository:
- "parSMURF1" is a fast multi-threaded implementation of the algorithm and is meant to be run on a single machine
- "parSMURFn" is a multi-threaded and parallel implementation (under the MPI programming paradigm) and is meant to be run on a single machine or on cluster

Both versions share the same design and functionalities outlined in the paper, in particular:
- fast, optimized and scalable C++ implementation
- auto tuning of the learning parameters by grid search or by means of a Bayesian optimizer
- ...


Requirements
------------
parSMURF is designed for x86-64 and Intel Xeon Phi architectures running Linux OSes.
This software is distributed as source code.

A compilier which supports the C++11 language specification is required. It has been tested with GCC (vers. >= 5) and Intel CC (2015, 2017 and 2019).
Code is also optimized for Intel XeonPhi architectures, and it has been successfully tested on Knights Landing family processors.

Multithreading and multiprocessing are managed differently in parSMURF1 and parSMURFn: the former is a multithread-only implementation and thread management is performed through OpenMP APIs. Any reasonably recent compiler has its specification already built-in, hence this requirement is usually met. parSMURFn, instead, is a multiprocess and multithread implementation of the algorithm. Thread management is performed by the Linux built-in pthread library and multiprocessing is performed through the MPI APIs.  Hence, for compilation and running, parSMURFn requires an implementation of the MPI standard. It has been tested with OpenMPI 1.10.3, OpenMPI 2.0, IntelMPI 2016, IntelMPI 2017 and IntelMPI 2019.

On Ubuntu, it is possible to install the OpenMPI library via apt package manager:
	sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev

Makefiles are generated by the cmake (vers. >= 2.8) utility. On Ubuntu it is possible to install this package via apt:
	sudo apt-get install cmake

Bayesian Optimization is done by the Spearmint package. This package require python2 and it depends on several Python packages. The best way to use this feature is by creating and configuring a Python virtual environment and installing the required Python packages there. On Ubuntu:
	sudo apt-get install virtualenv
	<move to an appropriate folder>
	virtualenv parSMURFvenv -p /usr/bin/python2		#This command should create a parSMURFvenv directory
	source parSMURFvenv/bin/activate				#This command activates the virtual environment
	pip install numpy==1.13.0						#The following commands install the required packages in the virtual environment
	pip install scipy==1.2.1
	pip install weave==0.17.0
	pip install six==1.12.0
	pip install protobuf==3.7.1
	deactivate										#Deactivate the virtual environment


parSMURF uses several external libraries that are included ad source code in this repository or are automatically downloaded and compiled. In particular, the following libraries are included:
- ANN: A Library for Approximate Nearest Neighbor Searching, by David M. Mount and Sunil Arya, Version 1.1.2. The modified version is supplied in the src/ann_1.1.2 directory. This version has been adapted for multi-thread execution, since the original package available at https://www.cs.umd.edu/~mount/ANN/ is not thread safe and is not compatible with this package.
- Ranger: A Fast Implementation of Random Forests, by Marvin N. Wright, Version 0.11.2. The modified version, stripped from the R code, is supplied in the src/ranger directory. The main codebase is located at https://github.com/imbs-hl/ranger
- Spearmint, a Python package to perform Bayesian optimization, by Jasper Snoek. The original version at https://github.com/JasperSnoek/spearmint seems no longer maintained and needed a few updates to run on parSMURF.

The following libraries are not included in this code repository, but are automatically downloaded during the compilation process:
- easylogging++: A single header C++ logging library, by Zuhd Web Services. Automatically cloned in src/easyloggingpp and compiled from https://github.com/zuhd-org/easyloggingpp
- jsoncons: A C++, header-only library for constructing JSON and JSON-like text and binary data formats, by Daniel Parker. Automatically cloned in src/jsoncons and compiled from https://github.com/danielaparker/jsoncons

All the libraries have been modified and redistributed according to their own licenses. For each included library, a copy of the associated license is contained in each library folder.


Downloading and compiling
-------------------------
Download the latest version from this page or clone the git repository altogether:

	git clone https://github.com/anacletolab/parSMURF


Once the package has been downloaded, move to the main directory, create a build dir, invoke cmake and build the software ("-j n" make option enables multithread compilation over n threads):

	cd parSMURF
	mkdir build
	cd build
	cmake ../src
	make -j 4

This should generate two executables: "parSMURF1" and "parSMURFn".




General architecture
--------------------
While both versions strictly follow the paper <link or citation> and its original R implementation (available on CRAN repository https://cran.r-project.org/web/packages/hyperSMURF/index.html), the novelties of this package resides in the fast C++ code and in the parallel execution which lead to a dramatic decrease of the computing time while keeping the same results, in term of quality of prediction, of the original implementation. Also, it features two different approaches for automatically find the best learning parameters.

Hence, execution roughly follows this scheme:
- data reading from file(s) (or random dataset generation)
- folds and partitions generation [by index!]
- for each fold
---- for each partition in the current fold
---- ---- over-sampling of the minority class and under-sampling of the majority class
---- ---- random forest training
---- ---- random forest test
---- prediction accumulation
- prediction averaging

Results are evaluated according to an n-fold validation process. Folds can be randomly generated (the user is free to specify the number of folds) or can be read from a file. When randomly generated, folds are stratified, i.e. the generation algorithm tries to evenly distribute the number of positive examples amongst the folds.

Parallelization happens at partition level: since the SMOTE algorithm and the subsequent RF train and test stages are almost embarrassingly parallel inside each fold, (i.e. they require the same operations to be performed on different data, with no synchronization points or data communication involved) these steps can be executed concurrently for each partition belonging to the same fold.

In parSMURF1, this process is parallelized by means of multi-threading. As an example, if the user specifies x partitions and y processing threads, each thread is assigned x/y partitions which are sequentially processed by each thread. If enough cpu cores are available, each thread will execute concurrently, leading to an almost linear speed-up, especially on CPUs characterized by an high number of cores, like the Intel XeonPhi family of processors.

Parallelization in parSMURFn follows the same model which is further expanded for exploiting the computational power of several processing nodes in a cluster. The execution scheme follows a simple master-slave model, where a single master MPI process reads the data from file and delegates the processing of each partition (SMOTE and rf steps) to k working MPI processes. The master process also manages the recollection and accumulation of the predictions from the working processes. Moreover, as in parSMURFa, processing of the partitions in each working process is parallelized by means of multi-threading.

As an example, suppose that the user specifies x partitions, k working processes and y processing threads for each working process.
The master process assigns x/k "chunks" of partitions to each working process and sends them the relevant data for the computation. Inside each working process, each chunk is further divided amongst the thread pool, and each thread is assigned to (x/k)/y partitions. Predictions for each chunks are locally accumulated inside each working process and are sent back to the master process only once the work for the chunk is finished.

Several strategies have been used to minimize latencies due to data transmission or broadcasting between the master and working processes, not limited to:
- the master process sends only the data strictly needed for the computation of each partition; moreover, it is sent as a single big array with an header, instead of several small chunks.
- sends and receives in the master process are managed in two different threads, hence interleaving data preparation + transmission and data receive.
- sends in the master process can be single- or multi-threaded: in the latter case, the master process spawns a number of threads equal to the number of working processes, and each of these thread is assigned to prepare the data and send it to the corresponding worker, concurrently. This is the default operation mode, but might be memory consuming, therefore a command line option to disable this feature is provided.

parSMURF features two subsystems for the automatic fine tuning of the learning parameters, aimed to maximize the prediction performances of the algorithm. The first strategy is by performing an exhaustive grid search: given a set of values for each hyper-parameter, the resulting set of all the possible combinations of hyper-parameters is calculated, and each combination evaluated through internal cross validation. The other strategy is by Bayesian optimization: given a range for each hyper-parameter, the Bayesian optimizer generate a sequence of possible candidates whose sequence tends to a probable global maximum. An high level of the execution is given by this pseudo-code snippet:

iter = 0
- while (iter < maxIter) and (error > tolerance):
-- BO generates a new possible candidate of hyper-parameters h
-- evaluation of h in a context of internal cross validation
-- submit (h, AUPRC(h)) to the BO
-- iter <- iter + 1

Both strategies are performed in a context of internal cross validation, hence it is performed for each fold of the external CV.
The output of the procedure is the set of best learning parameter for each fold of the external cross validation.


Configuration file
------------------
parSMURF1 and parSMURFn use configuration files in json format for setting the parameters of each run.
Examples of configuration files are available in the cfgEx folder of the repository.



Data format
-----------
As previously stated, data is provided to the application in two or three files.

Data file:
this file should contain the main data needed for computing the predictions. It consists in an n x m matrix of double, where n is the number of examples and m the features. The matrix is read column-wise, i.e. :

   | m1   m2   m3   m4 ...
---------------------------
n1 |  |
n2 |  |
n3 |  |
n4 | \ /                        ( 1 )
.  |  v
.  |
.  |

Hence, the data file should be ordered so that the first line contains all the examples for the first feature, the second line the examples for the second feature, and so on.
parSMURF1 and parSMURFn accepts data files in two different formats:
- plain text file: n x m space (or tab, comma, etc...) separated values
- binary file: binary encoded n x m values
Binary file format is highly recommended, since it is 5 to 10 times faster than parsing a plain text file. Data file must have a ".bin" extension so that parSMURF1 and parSMURFn import the data files in this mode.

The following code snippet converts a R matrix (called "data") in a binary file for proper use with this package, provided that the matrix is as in the example ( 1 ):

	outfile <- file( "binaryDataFile.bin", "wb" )
	for (i in 1:ncol( data )) {
        dataColumn = data[,i]
    	writeBin( dataColumn, outfile )
	}

Label file:
this file should contain the labelling of the examples. It consists in n space or tab separated values, where n is the number of examples.
It is a plain text file where each positive example is marked with "1" and negative examples with "0".

Fold file: this optional file should contain the fold sub division. If specified, examples will be divided in folds as specified in this file. If not, a random stratified division will be performed. This file consists in n space or tab separated integer values, where n is the number of examples.
It is a plain text file where each number represents the fold to which each example is assigned. Fold numbering starts from "0" (zero).
Note that specifying the fold file name overrides the "nFolds" option in the ocnfiguration file.

The following code snippet converts two R vectors in the corresponding labelling and folding files for proper use with this package:

	write(vectorOfLabels, file = "labels.txt", sep = "\n")
	write(vectorOfFolds, file = "folds.txt", sep = "\n")

Output file:
Predictions will be saved as plain text file.
The output file consists of two lines of n space separated double values, where n is the number of examples.
Each value in the first line represents the probability of the associated example to be in the minority class, while each value in the second line, the probability to be in the majority class.

Note about dimensionality:
When reading data from file, parSMURF1 and parSMURFn automatically detect the number of examples and features, following these rules:
- at first, the number of examples is detected from the label file.
- then, the number of features is detected from the data file and calculated as number of (elements read in the data file) / (number of labels).
Hence, the sizes of these files should be consistent, otherwise a warning message is printed to the console.
Also, the number of folds is detected from the fold file if specified. In this case, the option "nFolds" in the configuration file is ignored, and the total number of folds will be equal to the number of the total unique elements of the fold file.


Random dataset generation
-------------------------
parSMURF1 and parSMURFn are provided with a random dataset generator for testing purposes.
When enabled, a random dataset will be created according to two normal distribution having the same variance but different average value, depending if an example falls in the positive or negative class.
The user enables this mode by using the "--simulate <prob>" option, where <prob> is the probability that an example belongs to the minority class. Being a probability, this parameter must be 0 < prob <= 1.
The user is also forced to specify the dimensionality of the dataset with the "-n <#examples>" and "-m <#features>" options.
An additional line will be added to the output file, containing the labelling that has been randomly generated according to the <prob> value.


Examples
--------
<scrivere e commentare qualche file cfg di esempio>


License
-------
<file su Cudone... copia e incolla>
